<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Embodied Task Planning with Large Language Models">
    <meta name="author" content="Zhenyu Wu,
                                 Ziwei Wang,
                                 Xiuwei Xu,
                                 Jiwen Lu,
                                 Haibin Yan">

    <title>Embodied Task Planning with Large Language Models</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->


</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Embodied Task Planning with Large Language Models</h2>
    <h3>arXiv 2023</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://gary3410.github.io/">Zhenyu Wu</a><sup>1</sup>&nbsp;&nbsp;
        <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>2,3</sup>&nbsp;&nbsp;
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>2,3</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>2,3</sup>
        <a href="https://scholar.google.com/citations?hl=en-US&user=-AQLKlsAAAAJ">Haibin Yan</a><sup>1&#42;</sup>
        </span>
        <br><br>
        <sup>1</sup>School of Automation, Beijing University of Posts and Telecommunications, China<br>
        <sup>2</sup>Department of Automation, Tsinghua University, China<br>
        <sup>3</sup>Beijing National Research Center for Information Science and Technology, China<br>
        <br><br>
        <a href="https://arxiv.org/abs/2307.01848" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            Paper
        </a>&nbsp;

        <a href="https://github.com/Gary3410/TaPA" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code
        </a>
        &nbsp;
        <a href="https://huggingface.co/spaces/xuxw98/TAPA" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/picture.png" alt="code" style="vertical-align: middle;">
            Demo
        </a>
        &nbsp;
        &nbsp;
         <a href="https://huggingface.co/Gary3410/pretrain_lit_llama" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/robot.png" alt="code" style="vertical-align: middle;">
            Model
        </a>


    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>




<div class="container">




    <iframe
    src="https://xuxw98-tapa.hf.space"
    frameborder="1"
    width="900"
    height="1200"
    ></iframe>


    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for the agent in plan generation of complex tasks, while they lack the information about the realistic world and usually predict infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the names of objects in the scenes for GPT-3.5 to generate a large number of instructions and corresponding planned action steps. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scenes by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/demo.gif" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            Although multi-modal VLMs have achieved surprising performance on a wide range of fields, embodied task planning still remains a challenging task due to: 1) the lack of relevant datasets; 2) the requirement of simultaneous scene understanding and reasoning.Considering the recent success of GPT models on high-level human-like reasoning, we propose to represent the embodied scenes with texts and leverage ChatGPT/GPT-4 for data generation.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.jpg" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            <b>Illustration of TaPA.</b>  We first collect multiple RGB images in different achivable standing points and views, and utilize an open-voculary detector to generate the list of existing objects in the scene. With the human instructions and predicted objectlist, our TaPA can generate executable action plans for subsequent navigation or manipulation robots.
        </p>
    </div>

    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
             We conduct extensive experiments with our generated multimodal dataset where the visual scenes come from the simulator AI2-THOR.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment2.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
             Comparison of different LLMs and LMMs on the task of embodied task planning. For the prompt of baseline methods, LLaMA and LLaVA both employ the same prompt of TaPA in the training phase, while GPT-3.5 adopts the same prompt of TaPA for multimodal data generation.
        </p>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/experiment1.png" alt="pipeline" width="50%">
            </div>
        </div>
        <p>
            The percentage of different failure cases in embodied task planning for different action step generation methods.Obviously, our method outperforms the comparative LLMS in both metrics in the figure.
        </p>
    </div>


    <div class="section">
        <h2>Qualitative Results</h2>
        <hr>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/results.jpg" alt="pipeline" width="100%">
            </div>
        </div>
        <p>
            Results of different baseline parsing human abstract instructions are demonstrated. LLaMA, and GPT-3.5 inputs are all the perception results. TaPA inputs are the series of surround view images. LLaVA inputs are only one image. Object List represents the objects labeled by AI2-THOR in that scene (GT).
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
        @article{TaPA,
            title={Embodied Task Planning with Large Language Models},
            author={Zhenyu Wu and Ziwei Wang and Xiuwei Xu and Jiwen Lu and Haibin Yan},
            journal={arXiv preprint arXiv:2305.03716},
            year={2023}
        }
            </div>
        </div>
    </div>

    <hr>

    <footer>
        <!-- <h6>Acknowledgement</h6> -->
        <p><small>The website template was borrowed from <a href="https://xuxw98.github.io/DSPDet3D/">Xiuwei Xu</a></small></p>
    </footer>


    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=KFxiSzNXdwhkw0Znzcat5j6WJhvziaXtw3yEgmx6Y8c&cl=ffffff&w=a"></script>
        </div>        
        <br>
        &copy; Jiangfan Ran | Last update: July. 2, 2023
    </center>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
